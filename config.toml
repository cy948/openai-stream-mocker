[general]
default_tokens_per_second = 10
default_model_description = "Default configuration"
max_stream_time_seconds = 120
enforce_time_limit = false
truncation_message = "\n\nI've reached the response time limit, but I hope this information helps. Let me know if you need more details."
default_response_mode = "auto"

# Define auto response length thresholds
[auto_response_length]

[auto_response_length.slow]
max_tokens_per_second = 8.0
response_length = "short"

[auto_response_length.medium]
max_tokens_per_second = 20.0
response_length = "medium"

[auto_response_length.long]
max_tokens_per_second = 30.0
response_length = "long"

[auto_response_length.fast]
max_tokens_per_second = 80.0
response_length = "very_long"

# Model configurations
[models]

[models.llama-3-8b]
tokens_per_second = 150
description = "Llama 3 (8B parameters)"
parameters = 8

[models.llama-3-70b]
tokens_per_second = 70
description = "Llama 3 (70B parameters)"
parameters = 70

[models.codellama-13b]
tokens_per_second = 100
description = "Code Llama (13B parameters)"
parameters = 13

[models.codellama-34b]
tokens_per_second = 90
description = "Code Llama (34B parameters) - Full output mode"
parameters = 34
max_stream_time_seconds = 180  # Extended time limit for this specific model

[models.deepseek-r1]
tokens_per_second = 20
description = "DeepSeek R1 (>600B parameters)"
parameters = 10

[models.gpt-3.5-turbo]
tokens_per_second = 15
description = "GPT-3.5 Turbo"

[models.gpt-3.5-turbo-16k]
tokens_per_second = 10
description = "GPT-3.5 Turbo with 16k context window"
parameters = 20

[models.gpt-4]
tokens_per_second = 7.5
description = "GPT-4"

[models.gpt-4-turbo]
tokens_per_second = 7
description = "Enhanced GPT-4 Turbo model"
parameters = 140

[models.gpt-4-32k]
tokens_per_second = 5
description = "GPT-4 with 32k context window"
parameters = 170

[models.claude-2]
tokens_per_second = 20
description = "Anthropic Claude 2"

[models.claude-instant]
tokens_per_second = 12
description = "Faster Claude model"
parameters = 60

# Add your 34b model
[models.your-34b-model]
tokens_per_second = 90
description = "High-speed 34B model"
parameters = 34

# Response templates
[responses.short]
content = """
This is a short response from the OpenAI Stream Mocker.
Update the default time limit values to give more generous limits for high-speed models:

$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$

Uses a simple timer approach by checking the elapsed time on each iteration
"""

[responses.medium]
content = """This is a medium-length response from the OpenAI Stream Mocker.

Uses a simple timer approach by checking the elapsed time on each iteration
When the time limit is exceeded, it completes the current paragraph and adds the truncation message
Removes all the complex time calculation logic that was trying to predict completion time
Adjusts the default time limit to 120 seconds, with a 180-second override for the codellama-34b model
Adds a new endpoint for fine-grained control of time limits per model

$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$
It contains several paragraphs with in-depth explanations, examples, and reasoning. This is designed to simulate lengthier AI assistant responses that would be generated for complex questions.

The response includes some technical details and explanations to mimic AI assistant behavior."""

[responses.long]
content = """
This is a comprehensive response from the OpenAI Stream Mocker that covers multiple aspects of the topic in detail.
$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$
It contains several paragraphs with in-depth explanations, examples, and reasoning. This is designed to simulate lengthier AI assistant responses that would be generated for complex questions.
The structure includes introduction, main points, supporting details, and conclusions - similar to well-formatted AI responses.
$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$

Simplify Time Limit Implementation
Let's change the time limit implementation to use a simple timer approach instead of calculating text length. This will make the behavior more predictable and ensure we don't truncate content prematurely.

Step-by-Step Solution
Modify the stream_response function in services.py to use a timer-based approach
Remove complex time calculation logic
Add a simple timeout mechanism

Technical terminology and varied sentence structures are included to make the response appear more natural and sophisticated.
Modern applications increasingly rely on sophisticated language models provided through APIs like OpenAI's GPT series. Testing these integrations presents unique challenges:

1. **Cost Considerations**: API calls to commercial language models incur charges, making extensive testing expensive.
2. **Rate Limitations**: Most API providers impose rate limits that restrict testing throughput.
3. **Variability in Responses**: Language models produce different outputs for the same input, complicating test result validation.
4. **Streaming Response Handling**: Applications must correctly process chunked streaming responses, which is difficult to test systematically.
5. **Network Dependencies**: Testing requires constant internet connectivity and is affected by network latency.

### Implementation Details

The server is built using FastAPI, a modern, high-performance web framework for Python that makes it particularly suitable for API development. The streaming functionality is implemented using asynchronous generators, which efficiently produce token chunks at controlled intervals.

The system uses a token estimation algorithm to approximate real-world tokenization, ensuring that response timing closely mimics actual API behavior. This approach balances simplicity with accuracy, providing a testing environment that feels authentic without the complexity of implementing a full tokenizer.

## Advanced Usage Patterns

### Scenario-Based Testing

OpenAI Stream Mocker can be used to create repeatable test scenarios:

```python
# Example of setting up a specific test scenario
import requests
# Configure a slower model for UI testing
requests.post("http://localhost:8000/config", json={
    "model": "test-slow-model",
    "tokens_per_second": 2,
    "description": "Deliberately slow model for UI testing"
})
# Test application with the configured model
response = requests.post("http://localhost:8000/v1/chat/completions", json={
    "model": "test-slow-model",
    "messages": [{"role": "user", "content": "Test prompt"}],
    "stream": True
})
# Process streaming response
for line in response.iter_lines():
    if line:
        # Process each chunk as it arrives
        process_chunk(line)
```


"""

[responses.very_long]
content = """
[responses.very_long]
# Comprehensive Guide to Language Model Testing with OpenAI Stream Mocker
## Introduction to API Testing Challenges

Modern applications increasingly rely on sophisticated language models provided through APIs like OpenAI's GPT series. Testing these integrations presents unique challenges:

1. **Cost Considerations**: API calls to commercial language models incur charges, making extensive testing expensive.
2. **Rate Limitations**: Most API providers impose rate limits that restrict testing throughput.
3. **Variability in Responses**: Language models produce different outputs for the same input, complicating test result validation.
4. **Streaming Response Handling**: Applications must correctly process chunked streaming responses, which is difficult to test systematically.
5. **Network Dependencies**: Testing requires constant internet connectivity and is affected by network latency.

OpenAI Stream Mocker was developed specifically to address these challenges by providing a local, controllable testing environment for applications that integrate with language model APIs.
$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$

## Technical Architecture

The OpenAI Stream Mocker implements a server that faithfully replicates the behavior of OpenAI's API endpoints while offering extensive configuration options:

### Core Components

1. **API Endpoint Simulation**: Implements the same request/response patterns as OpenAI's official endpoints.
2. **Token Generation Engine**: Simulates the incremental production of tokens at configurable rates.
3. **Model Configuration System**: Allows detailed customization of response characteristics per model.
4. **Usage Statistics**: Tracks and reports token usage in the same format as the original API.

### Implementation Details

The server is built using FastAPI, a modern, high-performance web framework for Python that makes it particularly suitable for API development. The streaming functionality is implemented using asynchronous generators, which efficiently produce token chunks at controlled intervals.

The system uses a token estimation algorithm to approximate real-world tokenization, ensuring that response timing closely mimics actual API behavior. This approach balances simplicity with accuracy, providing a testing environment that feels authentic without the complexity of implementing a full tokenizer.

## Advanced Usage Patterns

### Scenario-Based Testing

OpenAI Stream Mocker can be used to create repeatable test scenarios:

```python
# Example of setting up a specific test scenario
import requests

# Configure a slower model for UI testing
requests.post("http://localhost:8000/config", json={
    "model": "test-slow-model",
    "tokens_per_second": 2,
    "description": "Deliberately slow model for UI testing"
})

# Test application with the configured model
response = requests.post("http://localhost:8000/v1/chat/completions", json={
    "model": "test-slow-model",
    "messages": [{"role": "user", "content": "Test prompt"}],
    "stream": True
})

# Process streaming response
for line in response.iter_lines():
    if line:
        # Process each chunk as it arrives
        process_chunk(line)
```

$$
L_{\\delta}(y, \\hat{y}) = \\begin{cases}
\\frac{1}{2}(y - \\hat{y})^2 & \\text{for } |y - \\hat{y}| \\leq \\delta, \\\\
\\delta (|y - \\hat{y}| - \\frac{1}{2} \\delta), & \\text{otherwise.}
\\end{cases}
$$


### Integration with Testing Frameworks

The mock server can be integrated with popular testing frameworks:

```python
# Example using pytest with OpenAI Stream Mocker
import pytest
import subprocess
import time
import requests

@pytest.fixture(scope="session")
def mock_server():
    # Start the mock server as a subprocess
    process = subprocess.Popen(["python", "main.py"])
    time.sleep(1)  # Wait for server to start
    
    yield
    
    # Terminate the server after tests
    process.terminate()

def test_streaming_response_handler(mock_server):
    # Test that our application correctly processes streaming responses
    response = requests.post("http://localhost:8000/v1/chat/completions", 
                           json={"model": "gpt-3.5-turbo", "messages": [...], "stream": True})
    
    # Test application's handling of the streaming response
    result = my_app.process_streaming_response(response)
    assert result.is_complete
    assert len(result.content) > 0
```

## Performance Considerations

When simulating high-volume or high-speed streaming responses, consider the following:

1. **Server Resources**: The mock server's performance depends on available CPU resources. For high-speed simulations, ensure the host machine has adequate processing power.

2. **Network Conditions**: Even when testing locally, network stack limitations can affect streaming performance. Use loopback interfaces for the highest throughput.

3. **Client Buffering**: Testing extremely high token generation rates may reveal buffering issues in client applications that wouldn't appear with actual API calls.

## Best Practices for Effective Testing

1. **Match Production Settings**: Configure the mock server to match the production environment's expected token generation rates.

2. **Test Edge Cases**: Use the mock server to simulate extremely slow or fast responses, which might be rare but possible in production.

3. **Automated Regression Testing**: Incorporate the mock server into CI/CD pipelines to automatically test API integration with each code change.

4. **Response Variety**: Use different response length presets to test how the application handles varying content sizes.

5. **Error Simulation**: Test application resilience by configuring the mock server to occasionally return error responses.

By following these practices, development teams can build more robust applications that gracefully handle the variabilities inherent in language model API interactions.

## Conclusion

OpenAI Stream Mocker represents a valuable addition to the toolkit of developers working with language model APIs. By providing a controlled, configurable environment for testing streaming API interactions, it enables more thorough testing, faster development cycles, and ultimately more reliable applications.

As language models continue to become central components in modern software, tools like OpenAI Stream Mocker will be essential for maintaining high quality standards while managing the unique challenges these integrations present."""
